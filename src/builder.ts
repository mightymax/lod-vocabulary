import prefixes, { shrink } from '@zazuko/prefixes';
import fs from 'node:fs';
import { QueryEngine } from '@comunica/query-sparql';
import { DataFactory as factory, Store, Parser } from 'n3';
import { CustomPrefixes } from '@zazuko/prefixes/prefixes';
import reservedWords from './reservedWords.js';
const engine = new QueryEngine();

const customPrefixes: CustomPrefixes = {
  memorix: 'http://memorix.io/ontology#',
  sdo: 'https://schema.org/',
  xsd: prefixes.xsd
};

for (const prefix of Object.keys(customPrefixes)) {
  prefixes[prefix] = customPrefixes[prefix];
}

const wanted = process.argv.splice(2);
const filterWanted = (prefix: string) => wanted.length > 0 ? wanted.includes(prefix) : true;
const missingVocabularies: CustomPrefixes = {};

const getDataset = async (prefix: string) => {
  if (Object.hasOwn(customPrefixes, prefix)) {
    const quadFile = `./extra-vocabularies/${prefix}.nq`;
    if (!fs.existsSync(quadFile)) {
      missingVocabularies[prefix] = customPrefixes[prefix];
      console.error(quadFile + ' not found');
      return;
    }
    return new Parser({ factory, format: 'application/n-quads' }).parse(
      fs.readFileSync(quadFile, 'utf8'),
    );
  } else {
    if (!fs.existsSync(`node_modules/@vocabulary/${prefix}/index.js`)) {
      missingVocabularies[prefix] = prefixes[prefix];
      console.error(`- ${prefix}: ${prefixes[prefix]} (vocab NQ file is missing)`);
      return;
    } else {
      return (await import(`@vocabulary/${prefix}/index.js`)).default({
        factory,
      });
    }
  }
};

const prefixAsSafeTSObjectName = (prefix: string) =>
  reservedWords.includes(prefix.split('-').join('_'))
    ? `$${prefix.split('-').join('_')}`
    : prefix.split('-').join('_');

for (const prefix of Object.keys(prefixes).filter(filterWanted)) {
  const cleanPrefix = prefixAsSafeTSObjectName(prefix);
  let ts = `// This file is generated by the build script. Do not edit it manually.
import { prefixer as $prefixer } from '../utilities.js';
const prefixer = $prefixer('${prefixes[prefix]}');

/**
 * [${prefix}](${prefixes[prefix]}})
 */
const ${cleanPrefix} = {
`;

  const dataset = await getDataset(prefix);
  if (!dataset) {
    continue;
  }
  const store = new Store(dataset);
  const rq = fs
    .readFileSync('query.rq', 'utf8')
    .replace('<$GRAPH>', `<${prefixes[prefix]}>`);
  const bindingsStream = await engine.queryBindings(rq, { sources: [store] });
  let count = 0;
  for await (const bindings of bindingsStream) {
    count++;
    const term = shrink(bindings.get('item')!.value).replace(`${prefix}:`, '');
    const description = (bindings.get('description')?.value ?? '')
      .split('\n')
      .map((line: string) => `    * ${line.trim()}`)
      .join('\n');
    ts += `  /**
    * **[${bindings.get('label')?.value ?? term}](${bindings.get('item')!.value})** (${bindings.get('itemType')?.value})
    * ${description ? `\n${description}` : ''}
    */
    ${term.match(/^[a-z][a-z0-9_]+$/i) ? term : `'${term}'`}: prefixer('${term}'),

  `;
  }
  ts += `
  };

export default ${cleanPrefix}

`;
  if (count > 0) {
    fs.writeFileSync(`./src/vocabularies/${prefix}.ts`, ts, 'utf8');
  } else {
    missingVocabularies[prefix] = prefixes[prefix];
    console.error(
      `- ${prefix}: ${prefixes[prefix]} (no bindings for SPARQL query)`,
    );
  }
}

if (wanted.length === 0) {
  let ts = `
  // This file is generated by the build script. Do not edit it manually.
  `;
  for (const prefix of Object.keys(prefixes)) {
    if (!fs.existsSync(`./src/vocabularies/${prefix}.ts`)) {
      continue;
    }
    ts += `export { default as ${prefixAsSafeTSObjectName(prefix)} } from './vocabularies/${prefix}.js';\n`;
  }
  ts += `export const missingVocabularies: string[] = [${Object.keys(missingVocabularies)
    .map((key) => `'${key}'`)
    .join(', ')}];\n`;
  fs.writeFileSync(`./src/index.ts`, ts, 'utf8');
} else {
  console.warn('No new index.ts was generated');
}
